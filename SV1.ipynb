{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd053fd-289e-4830-a3eb-64f74103a878",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261d466-fcd3-4448-a923-fa2ee89df959",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c054f-901e-4b2d-95ee-1cc8f005d6f6",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb651cfa-6d43-4be5-9167-b34ea2685fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The objective function of a linear SVM aims to maximize the margin between the two classes while minimizing classification errors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee96e1e-5d1b-49fc-91d3-a39e8b93cfc5",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e351b8a-f872-4a2a-b965-4ed21023ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " The kernel trick in SVM allows the algorithm to operate in a high-dimensional, implicit feature space without ever computing the \n",
    " coordinates of the data in that space. It uses kernel functions to compute the dot product between the images of all pairs of data in the\n",
    " feature space. Common kernels include the polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel.\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bac8-b2ed-459a-8d2c-3021945bb2b8",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ddf99-4995-4006-aae3-b9985cab006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Support vectors are the data points that lie closest to the decision boundary (or margin) and are crucial in defining the position and \n",
    " orientation of the boundary. They are the critical elements of the training set because they directly influence the decision surface.\n",
    " For example, in a binary classification problem with two classes, the support vectors are the points that if removed, would change the \n",
    " position of the dividing hyperplane.\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25a15b-4bc0-4ecf-9fcc-fa3844e15b9d",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf682a5b-3424-4099-9a9f-b89dc63957c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperplane: In a two-dimensional space, the hyperplane is a line that separates the classes. In higher dimensions, it is a flat affine subspace of one dimension less than the ambient space.\n",
    "Marginal Plane: These are the planes that are parallel to the hyperplane and pass through the support vectors. The distance between these two planes is the margin.\n",
    "Soft Margin: Soft margin SVM allows some misclassifications to achieve better generalization. It introduces slack variables to tolerate points within the margin or on the wrong side of the hyperplane.\n",
    "Hard Margin: Hard margin SVM does not allow any misclassifications. It requires the data to be linearly separable with no points within the margin.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
